# -*- coding: utf-8 -*-
"""summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eosAv70RUrn3ptkG3eAwfk9b70z4QSYD

# Colab output wrapper

# Install Transformers
"""

from bs4 import BeautifulSoup
import requests

# Start a session
session = requests.Session()

# Define the headers
headers = {
    "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "en-GB,en-US;q=0.9,en;q=0.8,es;q=0.7",
    "Cookie": "GU_mvt_id=546190; bwid=idFromPV_tGNE1Y4ziW6RF9ZU7oKWAQ; bwid_withoutSameSiteForIncompatibleClients=idFromPV_tGNE1Y4ziW6RF9ZU7oKWAQ; consentUUID=7e54c557-2b08-429f-9b3a-62d04932b3aa_22; consentDate=2023-08-15T12:41:50.817Z; _ga=GA1.2.1086360360.1692103312; _gid=GA1.2.362089074.1692103312; permutive-id=e6896ed3-6a89-426c-bced-1b3e2f395993; _cc_id=6b76286e9308ea51392d6993ac96cd0b; panoramaId_expiry=1692708112890; panoramaId=8b4cbd9cd4e1289b855afdf3abb74945a7027a222951e8665464c8751b3a5aeb; panoramaIdType=panoIndiv",
    "Referer": "https://www.theguardian.com/books/2022/nov/05/i-want-to-open-a-window-in-their-souls-haruki-murakami-on-the-power-of-writing-simply",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
}

# Update the session with the headers
session.headers.update(headers)

# Now you can make requests with this session and the headers will be used automatically
response = session.get("https://www.theguardian.com/books/2022/nov/05/i-want-to-open-a-window-in-their-souls-haruki-murakami-on-the-power-of-writing-simply")

# Parse the content with BeautifulSoup
soup = BeautifulSoup(response.content, 'html.parser')

# Initialize the variable that will contain the full content
paragraphs = []

# Extract the title and add it to the content
title = soup.find('h1')
if title:
    paragraphs.append(title.get_text())

# Iterate through the article content and add each element in the order it appears
for element in soup.find_all(['h2', 'blockquote', 'p']):
    paragraphs.append(element.get_text())

print(paragraphs)

# install transformers with sentencepiece
!pip install transformers[sentencepiece]

"""# Read input file from Google Drive"""

# open and read the file from google drive
FileContent = '\n'.join( paragraphs)

# total characters in the file
len(FileContent)

"""# Load the Model and Tokenizer"""

# import and initialize the tokenizer and model from the checkpoint
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

checkpoint = "sshleifer/distilbart-cnn-12-6"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

"""# Some model statistics"""

# max tokens including the special tokens
tokenizer.model_max_length

# max tokens excluding the special tokens
tokenizer.max_len_single_sentence



# number of special tokens
tokenizer.num_special_tokens_to_add()

"""# Convert file content to sentences"""

# extract the sentences from the document
import nltk
nltk.download('punkt')
sentences = nltk.tokenize.sent_tokenize(FileContent)

# find the max tokens in the longest sentence
max([len(tokenizer.tokenize(sentence)) for sentence in sentences])

"""# Create the chunks"""

# initialize
length = 0
chunk = ""
chunks = []
count = -1
for sentence in sentences:
  count += 1
  combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter

  if combined_length  <= tokenizer.max_len_single_sentence: # if it doesn't exceed
    chunk += sentence + " " # add the sentence to the chunk
    length = combined_length # update the length counter

    # if it is the last sentence
    if count == len(sentences) - 1:
      chunks.append(chunk.strip()) # save the chunk

  else:
    chunks.append(chunk.strip()) # save the chunk

    # reset
    length = 0
    chunk = ""

    # take care of the overflow sentence
    chunk += sentence + " "
    length = len(tokenizer.tokenize(sentence))
len(chunks)

"""# Some checks"""

[len(tokenizer.tokenize(c)) for c in chunks]

[len(tokenizer(c).input_ids) for c in chunks]

"""## With special tokens added"""

sum([len(tokenizer(c).input_ids) for c in chunks])

len(tokenizer(FileContent).input_ids)

"""## Without special tokens added"""

sum([len(tokenizer.tokenize(c)) for c in chunks])

len(tokenizer.tokenize(FileContent))

"""# Get the inputs"""

# inputs to the model
inputs = [tokenizer(chunk, return_tensors="pt") for chunk in chunks]

"""# Output"""

for input in inputs:
  output = model.generate(**input)
  print(tokenizer.decode(*output, skip_special_tokens=True))